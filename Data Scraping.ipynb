{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c835d9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839f5352",
   "metadata": {},
   "source": [
    "### Helper Functions for Data Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067319e3",
   "metadata": {},
   "source": [
    "For this Data Analysis on Canada Recalled Items (2011-2023), we will extract data from the [Recalls and safety alerts - Government of Canada](https://recalls-rappels.canada.ca/en/search/site) website using web scraping technique. Here are the functions for collecting data:\n",
    "\n",
    "\n",
    "- `retrieve_data_from_version1(page)`\n",
    "- `retrieve_data_from_version2(page)`\n",
    "- `retrieve_data(page)`\n",
    "- `get_webpage_content(link)`\n",
    "- `get_all_links(category)`\n",
    "- `collect_data(start, end, all_links)`\n",
    "\n",
    "The webpage has 2 different layouts, which makes the structure of the content unconsistent. To be able to scrape all the data, we will need the two following methods `retrieve_data_from_version1(page)` for the old layout version and `retrieve_data_from_version2(page)` for the recent layout version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b73eedf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_data_from_version1(page):\n",
    "    \"\"\"\n",
    "    This function extracts informations from the given webpage that has the old layout version (version 1)\n",
    "    \n",
    "    :param [page] : content of the webpage for data extraction\n",
    "    :type [page] : BeautifulSoup\n",
    "    \n",
    "    :return : dictionary with extracted data \n",
    "    :rtype : dictionary\n",
    "    \n",
    "    \"\"\"\n",
    "    # create dictionary\n",
    "    data = {}\n",
    "    data['Date'] = page.find_all('time')[-1].text\n",
    "    data['Item'] = page.find('h1').text\n",
    "    category = page.find('div', {'class':'h3'}).text.lower() \n",
    "    data['Recall type'] = category\n",
    "    \n",
    "    # locate our features and values section in the webpage\n",
    "    features = page.find_all('dt', {'class' : 'paddingNone'})\n",
    "    values = page.find_all('dd', {'class' : 'paddingNone'})\n",
    "    \n",
    "    for f, v in zip(features, values):\n",
    "        data[f.text] = v.text\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "386d86c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_data_from_version2(page):\n",
    "    \"\"\"\n",
    "    This function extracts informations from the given webpage that has the recent layout version (version 2)\n",
    "    \n",
    "    :param [page] : content of the webpage for data extraction\n",
    "    :type [page] : BeautifulSoup\n",
    "    \n",
    "    :return : dictionary with extracted data\n",
    "    :rtype : dictionary\n",
    "    \n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    features, informations, date = None, None, None\n",
    "\n",
    "    # Locate the 'Summary' and 'Additional Information' sections\n",
    "    summary_section = page.find('div', {'class':'alert-info'}).find_all('div', 'field--item')\n",
    "    additional_section = page.find('div', {'class': 'ar-additional-info'}).find_all('details')\n",
    "    \n",
    "\n",
    "    # Locate the 'Details' section\n",
    "    for i in range(len(additional_section)):\n",
    "        \n",
    "        if additional_section[i].find('summary').text == 'Details': \n",
    "            details = additional_section[i]\n",
    "            \n",
    "            # Extract the informations from 'Details' section\n",
    "            features = details.find_all('div', {'class': 'field--label'})\n",
    "            informations = details.find_all('div', {'class': 'field--item'})\n",
    "            break\n",
    "            \n",
    "    # Retrieve brand from the website\n",
    "    if page.find('details', {'class':'ar-brand-details'}):\n",
    "        brand = page.find('details', {'class':'ar-brand-details'}).find('div', 'field--item').text\n",
    "    else:\n",
    "        brand = np.nan\n",
    "    \n",
    "    # Retrieve number of affected items/models\n",
    "    if page.find_all('table'): \n",
    "        nb_affected_products = len(page.find_all('table')[0].select('tr'))\n",
    "    elif page.find_all('div', {'class': 'ar-affected-products'})[0].text != '\\n':\n",
    "        items = page.find_all('div', {'class': 'ar-affected-products'})\n",
    "        nb_affected_products = len(items[0].select('div', {'class':'field--item'})[0].get_text().split(','))    \n",
    "    else:\n",
    "        nb_affected_products = 1\n",
    "    \n",
    "    \n",
    "    # create dictionary\n",
    "    data['Date'] = page.find_all('time')[-1].text\n",
    "    data['Item'] = page.find('h1').text\n",
    "    data['Brand'] = brand\n",
    "    data['Product'] = summary_section[0].text\n",
    "    data['Issue'] = summary_section[1].text\n",
    "    data['Nb_affected_models'] = nb_affected_products\n",
    "    \n",
    "    for f, info in zip(features[:-2], informations[:-2]):\n",
    "        info = info.text.lstrip().rstrip()\n",
    "        data[f.text] = info\n",
    "\n",
    "        \n",
    "    category = page.find('div', {'class':'h3'}).text.lower() \n",
    "    data['Recall type'] = category\n",
    "    if 'food' in category or 'vehicle' in category:\n",
    "        data[features[-3].text] = informations[-4].text\n",
    "    else:\n",
    "        data[features[-2].text] = informations[-2].text\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed941726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_data(page):\n",
    "    \"\"\"\n",
    "    This function extracts informations from the given webpage by checking which layout version it has.\n",
    "    \n",
    "    :param [page] : content of the webpage for data extraction\n",
    "    :type [page] : BeautifulSoup\n",
    "    \n",
    "    :return : dataframe with all the extracted data from the given webpage\n",
    "    :rtype : pd.DataFrame\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    item = None\n",
    "    if page.find_all('div', {'class': 'recall-alert-body'}): \n",
    "        item = retrieve_data_from_version1(page)\n",
    "    else:\n",
    "        item = retrieve_data_from_version2(page)\n",
    "    \n",
    "    return pd.DataFrame([item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ebe7f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_webpage_content(link):\n",
    "    \"\"\"\n",
    "    This function fetches the webpage of the given link.\n",
    "    \n",
    "    :param [link] : url address of the webpage to fetch\n",
    "    :type [link] : string\n",
    "    \n",
    "    :return : content of the webpage\n",
    "    :rtype : BeautifulSoup\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(link, verify=False)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(e.response.text)\n",
    "    \n",
    "    return bs(response.content, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41885e3",
   "metadata": {},
   "source": [
    "### Get all the links and Save to JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d53b3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_links(category):\n",
    "    \"\"\"\n",
    "    This function gets all the links of the https://recalls-rappels.canada.ca website for the given recall category\n",
    "    \n",
    "    :param [page] : recall category\n",
    "    :type [page] : string\n",
    "    \n",
    "    :return : links \n",
    "    :rtype : list of strings\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    id_category = {'vehicles': 443, 'consumer': 101, 'medical': 180, 'food': 144}\n",
    "    all_sublinks = []\n",
    "\n",
    "    # fetch the website\n",
    "    url = lambda page: f'https://recalls-rappels.canada.ca/en/search/site?f%5B0%5D=category%3A{id_category[category]}&search_api_fulltext=&archived=0&page=0%2C{page}'\n",
    "    content = get_webpage_content(url(0))\n",
    "        \n",
    "    # get total number of pages\n",
    "    pagination = content.find_all('li', {'class': 'pager__item'})     # locate pagination bar at the website bottom \n",
    "    last_page_link = pagination[-1].find('a')['href']                 # get the link to the last page\n",
    "    index = last_page_link.find('2C')                                 # locate the page number\n",
    "    total_pages = int(last_page_link[index + 2:])                     # extract the total number of pages from the link             \n",
    "        \n",
    "    # get all the links\n",
    "    url_base ='https://recalls-rappels.canada.ca'\n",
    "    for page in range(total_pages):\n",
    "        page_content = get_webpage_content(url(page))\n",
    "        for link in page_content.find_all('a'):\n",
    "            sublink = str(link.get('href'))\n",
    "            if sublink.startswith('/en/alert-recall'):\n",
    "                all_sublinks.append(url_base + sublink)\n",
    "                    \n",
    "    print('Links all collected')\n",
    "    return all_sublinks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2856b4e",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "# get all the links\n",
    "medical_urls = get_all_links('medical')\n",
    "vehicles_urls = get_all_links('vehicles')\n",
    "food_urls = get_all_links('food')\n",
    "consumer_urls = get_all_links('consumer')\n",
    "\n",
    "\n",
    "#save the links to a JSON file\n",
    "with open('links_medical.json', 'w') as file:\n",
    "   json.dump(medical_urls, file)\n",
    "\n",
    "with open('links_vehicles.json', 'w') as file:\n",
    "   json.dump(vehicles_urls, file)\n",
    "\n",
    "with open('links_food.json.json', 'w') as file:\n",
    "   json.dump(food_urls, file)\n",
    "\n",
    "with open('links_consumer.json', 'w') as file:\n",
    "   json.dump(consumer_urls, file)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b7ffcd",
   "metadata": {},
   "source": [
    "### Retrieve data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bed0e55",
   "metadata": {},
   "source": [
    "```python\n",
    "# open the JSON file\n",
    "with open('data/raw_data/links_medical.json', 'r') as file:\n",
    "    medical_urls = json.load(file)\n",
    "    \n",
    "with open('data/raw_data/links_vehicles.json', 'r') as file:\n",
    "    vehicles_urls = json.load(file)\n",
    "\n",
    "with open('data/raw_data/links_food.json', 'r') as file:\n",
    "    food_urls = json.load(file)\n",
    "    \n",
    "with open('data/raw_data/links_consumer.json', 'r') as file:\n",
    "    consumer_urls = json.load(file)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d01cbd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(start, end, all_links):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function collects data from the given list of links (read from JSON file) by batch. \n",
    "    Due to the large size of data to collect, it would be helpful to collect data \n",
    "    by \"smaller\" batch by setting the start/end of the urls list.\n",
    "    \n",
    "    :param [page] : list of links to fetch\n",
    "    :type [page] : list\n",
    "    \n",
    "    :return : dataset with all data extracted from the given list of urls\n",
    "    :rtype : pd.DataFrame\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    all_items_df = pd.DataFrame()\n",
    "\n",
    "    for link in all_links[start: end]:\n",
    "\n",
    "        content = get_webpage_content(link)\n",
    "        items_df = retrieve_data(content)\n",
    "        all_items_df = pd.concat([all_items_df, items_df], axis = 0)\n",
    "        time.sleep(2)\n",
    "\n",
    "    print('Pages successfully fetched')\n",
    "    return all_items_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b018990",
   "metadata": {},
   "source": [
    "**NOTE:**\n",
    "- There is a total of 14,218 recalled items to collect from the website from 2011 to March 2023.\n",
    "- For this Data Analysis project, we scraped the data by batches of 1,000 urls at a time for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e0d4d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
